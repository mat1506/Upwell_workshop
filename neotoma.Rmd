---
title: "Análisis Demo de la base de datos NeotomaDB en Chile"
description: |
  Un repaso sobre la situación actual de las bases de datos del proyecto AbrocomaDB
author:
  - name: Matías Frugone-Álvarez 
    url: https://matfrugone.netlify.app
    affiliation: Postdoctorado UPWELL
    affiliation_url: https://matfrugone.netlify.app
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

This workshop is intended to introduce individuals to different ways of analysing data stored in the Neotoma Paleoecology Database and the `neotoma` package for R.

1.  Introduction to Neotoma Explorer for easy data exploration and extraction.
2.  Training in the use of the Neotoma R package to archive, access, and analyze paleoecological data.

Neotoma is a multiproxy paleo-database that stores a range of paleoecological & paleoenvironmental data. One of the strengths of Neotoma is the ability to compare different paleo-proxy data such as fossil pollen, diatoms, ostracodes, insects, charcoal, and geochemical data. In addition, the database is structured to allow the creation and storage of age models built on absolute dates derived from age control points and stratigraphic sections. Neotoma is a public-access, community-supported database that has emerged as the standard repository for Pliocene and Quaternary paleoecological data.

More teaching materials can be found [in Neotoma's educational resources](http://www.neotomadb.org/education/category/higher_ed/).

This workshop is available on GitHub, in the NeotomaDB [repository](https://github.com/NeotomaDB/Workshops/tree/master/IBSQuito19).

# Finding Data

## Explorer

### Getting Started

1.  Go to <http://www.neotomadb.org/> and click on the 'Explorer' picture, or navigate directly to the [Explorer App](http://apps.neotomadb.org/Explorer/)
2.  Pan (by dragging), or change the zoom so that your window is centered on Chile.

### Search for Data

### Tips and tricks before we start

1.  Show/Hide Search Results

    a.  We will accumulate many search layers - it may be getting a bit confusing. Find the icon that lets you show/hide/combine search layers and use it to hide or delete some of your searches.

2.  Rename searches

    a.  Searches from the "Basic" search window are automatically named, but not "Advanced" searches.Add names for your searches as you go at the bottom of the Search dialog.

#### Finding sites

1.  **Find a known site**

    a.  Using the Search dialog window (Metadata subtab;Collection type:Core;Database:Neotoma; Site Name), find "Laguna del Maule".
    b.  Once you've performed the search, click on the point that appears. A window will pop up with some information about this record.

2.  **Explore sites by geography**

    a.  Using the Search dialog window, first choose "dataset type = Pollen" at the top

    b.  Then, in the Space subtab, click "Search by extent"

        1.  Click the "Extent" dropdown menu and search by shape, select the rectangle by using SHIFT+pointer, then draw a rectangle on the map in your chosen region of Chile (for example, South of Chile).

3.  **Find all sites within a database** Chile and the region has a rich history in paleoecological reseach of fossil pollen data. Let's see how many are currently in Neotoma:

    a.  Search Window, Metadata subtab;Collection type: Core; Database field
    b.  Find all sites within the Latin American Pollen Database.

    -   Not that many sites from the region are in Neotoma currently - only 36 datasets across 36 sites

#### Find a Taxon

1.  **Search for a single taxon.**

    a.  Search Window, select Dataset type: pollen, and use Taxon name
    b.  Find all sites within the Latin American Pollen Database with **Chenopodioideae** records

2.  **Search for multiple taxa**

    a.  Let's re-search for **Chenopodioideae**, but make sure we've included all relevant records.

#### Find all charcoal and pollen records in the database

1.  **Search window, Dataset type = "Charcoal"**
2.  **Search window, Dataset type = "Pollen"**

#### Multi-Taxon Search

1.  **Find all sites with Chenopodioideae pollen between 10,000 and 500 years ago.**

    a.  For **Chenopodioideae**, first click on "Dataset Type" = "pollen"
    b.  Then use the Taxon Selection which you can use by clicking on the gears icon to the right of the 'Taxon' field in the 'Search' window.

    -   Enter 'Taxa group' = Vascular Plant, search for *Casuarina*, click the box next to Taxon to check all taxa, then uncheck taxa like "*Myrica/Casuarina*" or "*Casuarina/Corylus*".

    a.  Then click the 'Abundance/density' box and select \>5%.\
    b.  Fill in the desired age range, and choose records that 'intersects result age range'.
    c.  Finally, click 'Search'

2.  **Find all sites with *Nothofagus* between 10,000 and 500 years**

    a.  Use the 'Advanced Taxon Selection' which you can use by clicking on the gears icon to the right of the 'Taxon' field in the 'Search' window. b Enter 'Taxa group' = Vascular Plant, search for *Nothofagus*, click the box next to Taxon to check all taxa you want to include.
    b.  Set a relative abundance filter if you'd like (click the 'Abundance/density' box and select \>5%)\
    c.  Fill in the desired age range, and choose records that 'intersects result age range'.
    d.  Finally, click 'Search'

You can combine these two searches to find sites with either Casuarina OR Nothfagus, or restrict it to only those sites with both taxa.

### Download Data

1.  Find the icon in the menu that indicates 'View search results in tables'and click on it.
2.  Click on the Save icon to the right. The dataset will be saved as a text file in CSV (comma separated value) format.
3.  Open the downloaded CSV file in Excel or a text editor (e.g. Notepad, Wordpad) to look at it.

# Web API- Application Programming Interface

## What is a Web API?

1.  Set of protocols for building tools and applications that use a specific web service
2.  Often a structure for URL/URI formulation to make a query on a web database, such as Neotoma, iDigBio, or the Paleobiology Database. + URL = Uniform Resource Locator, or web address. + URI = Uniform Resource Identifier, or web address of a service, like an API.
3.  Calls are usually sent as URL `GET` statements: values appended to a URL after a `?` that are processed as parameters to be passed to the code running on the server. For example, we can think of <http://google.com/?q=bananas> as if it were an R function called `search()`, passed as `search(q="bananas")`.

Google has APIs for most of its products. You could write an R (or Python, etc.) script that would connect to the Google Calendar API to allow you to automatically change events on your calendar or report agenda items back to you in your own custom environment. [Hilary Parker](https://hilaryparker.com/) has a nice example for [adding sunsets to Google Calendar](https://hilaryparker.com/2014/05/27/sunsets-in-google-calendar-using-r/) using R.

APIs exist for many important biological and paleobiological databases. We will work through a quick example from Neotoma. Different databases/services produce different kinds of API returns. The simplest kinds are comma delimited text files, but many APIs are now returning JSON documents.

## More about JSON

At its most basic level, JSON transmits data objects in attribute-value pairs. It has come to replace XML, which was the previous standard for this sort of data transmission.

JSON is composed of objects, enclosed by curly brackets, which may have any number of attributes named in quotes, with values after a colon, separated by commas. You may also present an array, or an ordered collection of values, enclosed in square brackets.

Let's search the Neotoma DB via the API for *Casuarina*. Here is the example API call:

[http://api.neotomadb.org/v1/data/sampledata?taxonname=Casuarina\*]()

Notice when you make this call that the JSON is computer-friendly but not human-friendly (though this depends on browser). Try it again with the 'pretty' format tag:

[http://api.neotomadb.org/v1/data/sampledata?taxonname=Casuarina\*&format=pretty]()

You should be able to see the nested set of JSON objects, including the occurrences returned as comma-separated objects within `"data"`.

Try experimenting with the search, substituting different names.

JSON is becoming the standard for data transfer in web services. R has several packages for dealing with JSON-formatted data. We will use some examples from the package [`RJSONIO`](https://cran.rstudio.com/web/packages/RJSONIO/index.html). We will also use the package [`RCurl`](https://cran.rstudio.com/web/packages/Rcurl/index.html), which has functions to let you query APIs from within the R environment. The `neotoma` package is built around the newer [`httr`](https://cran.rstudio.com/web/packages/httr/index.html) package.

## More on the Neotoma DB API

Based on the *Casuarina* example, you can see that the example API call is reporting data from only part of the distributed database schema of Neotoma. In fact, the Neotoma API is designed around a set of different URLs, each of which allows a user to search a portion of the database. So, if you want the full Site information for locations with *Casuarina* present, you would have to search on *Casuarina* in the SampleData URI (as we did in the example), pull the DatasetID values from those returns, then search on the Dataset URI ([http://api.neotomadb.org/v1/data/datasets]()) for those DatasetIDs, which would, in turn, produce the SiteIDs, which you would then search on the Sites URI ([http://api.neotomadb.org/v1/data/sites]()). This sort of searching would be cumbersome if you were to do it by hand, but fortunately you can script a computer to do it for you. In fact, you don't have to write the scripts to do it, because they have already been constructed and provided to the community as the R [`neotoma`](https://cran.rstudio.com/web/packages/neotoma/index.html) package.

The `neotoma` package will the the subject of our next module, but bear in mind that the other APIs also have wrapper packages to simplify data calls in R. The PaleoBioDB has a package, `paleobioDB`, and iDigBio has a package, `ridigbio`. The Earth-Life Consortium was aimed at creating a single API and wrapping R package to access both PaleoBioDB and Neotoma at the same time, as well as linking to many online museum databases and iDigBio. We wanted to introduce you to the underlying architecture here so you would understand what these packages are doing, and would know that you can crack them open and hack your own solutions if you cannot get them to give you the data or format of data that you need for your work.

# The `neotoma` Package

Install the `neotoma` package, then add it to your programming environment

```{r neotoma, echo=TRUE, eval = TRUE}
# install.packages('devtools')
# devtools::install_github('ropensci/neotoma')
# Add the packages to your programming environment 
library(neotoma)
```

`neotoma` has three core commands: `get_site`, `get_dataset`, and `get_download`. The first two return metadata for sites and datasets; the latter returns data. See Goring *et al*. 2015 for a full description of the package and example code. This exercise is partially based on those examples.

### Finding sites

We'll start with `get_site`. `get_site` returns a `data.frame` with metadata about sites. You can use this to find the spatial coverage of data in a region (e.g. using `get_site` with a bounding box), or to get explicit site information easily from more complex data objects. Use the command `?get_site` to see all the options available.

You can easily search by site name, for example, finding "Laguna del Maule", a site in central Chile.

```{r sitelem, echo=TRUE, eval = TRUE}

maule_site <- get_site(sitename = 'Laguna del Maule%')

```

Examine the results


```{r printlem, echo=TRUE, eval = TRUE}
print(maule_site)
```

While `maule_site` is a `data.frame` it also has class `site`, that's why the print output looks a little different than a standard `data.frame`. That also allows you to use some of the other `neotoma` functions more easily.

By default the search string is explicit, but because older sites,
especially pollen sites entered as part of COHMAP, often had appended
textual information (for example `(CA:British Columbia)`), it's often good practice to first search using a wildcard character. For example, searching for "Marion" returns three sites:


```{r marionsite, echo=TRUE, eval = TRUE}

#results= "hide"
marion_site <- get_site(sitename = 'Marion%')
```

```{r marionprint, echo=TRUE, eval = FALSE}
print(marion_site)
```

You can also search by lat/lon bounding box. This one roughly corresponds to central Chile (A numeric vector c(lonW, latS, lonE, latN) representing the bounding box within which to search for sites. The convention here is to use negative values for longitudes west of Greenwich or longitudes south of the equator)

```{r chilesite, echo=TRUE, eval = TRUE}
#results= "hide"
ch_sites <- get_site(loc = c(-72, -35, -68, -31)) 
```

You can also search by geopolitical name or geopolitical IDs (`gpid`) stored in Neotoma. For a list of names and gpids, go to <http://api.neotomadb.org/apdx/geopol.htm>, or use the `get_table(table.name = "GeoPoliticalUnits")` command. This command works either with an explicit numeric ID, or with a text string:



```{r charsite, echo=TRUE, eval = TRUE}

# get all sites in Chile (gpid=7956)
CH_sites <- get_site(gpid = 825)

# get all sites in Wisconsin
AR_sites <- get_site(gpid = "Argentina")

```

`data.frame` stores vectors of equal length. The nice thing about a `data.frame` is that each vector can be of a different type (character, numeric values, *etc*.). In RStudio, you can use the Environment panel in upper right to explore variables.

We pointed out before that the object returned from `get_site` is both a `data.frame` and a `site` object. Because it has a special `print` method some of the information from the full object is obscured when printed. You can see all the data in the `data.frame` using `str` (short for *structure*):

```{r strmaule, echo=TRUE, eval = FALSE}

str(maule_site)

```

Let's look at the `description` field:

```{r descrch, echo=TRUE, eval = FALSE}

CH_sites$description

AR_sites$description

```

### Getting Datasets

The structure of the Neotoma data model, as expressed through the API, is roughly: "`counts` within `download`, `download` within `dataset`, `dataset` within `site`". So a `dataset` contains more information than a `site`, about a particular dataset from that site.

A site may have a single associated dataset, or multiple. The Neotoma function `get_dataset` returns a list of datasets containing the **metadata** for each **dataset**. We can pass output from `get_site` (site metadata) to `get_dataset`, even if `get_site` returns multiple sites. For example:

```{r getdatach, echo=TRUE, eval = TRUE}


all_chill <- neotoma::get_dataset(gpid = 825)

#Examine the results

print(all_chill)
```

Use get.tables('GeoPoliticalUnits') for a list of acceptable values, or link here: <http://api.neotomadb.org/apdx/geopol.htm>

```{r plotch, echo=TRUE, eval = TRUE}
plot(all_chill)
```

Make sure the leaflet package is installed using install.packages('leaflet'), and then let's check out what we get:

```{r plotmap, echo=TRUE, eval = TRUE, message = FALSE, warning = FALSE, cache = TRUE}
library(leaflet)

# We're going to use this multiple times I think, so let's make it a function:

leaflet_map <- function(dataset_in) {
  dataset_summary <- do.call(rbind, lapply(dataset_in, 
                        function(x){
                          # here we pull out the site information from the `dataset` objects:
                          data.frame(name = x$site.data$site.name,
                                     lat  = x$site.data$lat + runif(1, -0.005, 0.005),
                                     long = x$site.data$long + runif(1, -0.005, 0.005),
                                     type = x$dataset.meta$dataset.type)
                        }))
  
  # The leaflet package documentation uses piping.  For the sake of this tutorial, I won't.
  # First, define a color palette for the dataset type symbol plotting.
  pal <- colorFactor("Dark2", domain = levels(dataset_summary$type))
  
  # Now make the leaflet map, add base raster tiles and then add the markers for the records:
  map <- leaflet(data = dataset_summary)
  map <- leaflet::addTiles(map)
  map <- leaflet::addCircleMarkers(map, ~long, ~lat, 
                                   popup = ~paste0("Site: ", as.character(name), "<br>",
                                                   "Type: ", 
                                                   as.character(dataset_summary$type)),
                                   color = ~pal(type),
                                   stroke = FALSE, fillOpacity = 0.5)
  # You need to explicitly call the `map` object to make it appear!
  map
}

# Since that's all wrapped in a function, we can all it with any `dataset_list`:
leaflet_map(all_chill)
```


### Get_Download

The Neotoma function `get_download()` returns a `list` that stores a list of `download` objects - one for each retrieved `dataset`. Note that `get_download()` returns the actual (raw) data associated with each dataset, rather than a list of the available datasets, as in `get_dataset()` above. Each download object contains a suite of data for the samples in that dataset.

Using the `get_download()` will accept an object of class `dataset` (ie, `tas_dataset`), but also of class `site` (ie, `lagoon_sites`), since it will automatically query for the datasets associated in each site.

```{r downch, echo=TRUE, eval = FALSE}
CH_all <- get_download(CH_sites)
#print(CH_all)
```

There are a number of messages that appear. These can be suppressed with the flag `verbose = FALSE` in the function call. One thing you'll note is that not all of the datasets can be downloaded directly to a `download` objct. This is because `geochronologic` datasets have a different data structure than other data, requiring different fields, and as such, they can be obtained using the `get_geochron()` function.

```{r chronoch, echo=FALSE, eval = FALSE}

#CH_geochron <- get_geochron(CH_sites)

#print(CH_geochron)
```

The result is effectively the inverse of the first.

Get the datasets for just El Maule (dataset 19817):

```{r downmaule, echo=TRUE, eval = FALSE}
maule_pd <- get_download(19817)
```

Let's examine the available data in this download

```{r, seemaule, echo=TRUE, eval = FALSE}
str(maule_pd[[1]])
```

There are 6 associated fields:

1.  dataset

    -   site.data
    -   dataset.meta
    -   pi.data
    -   submission
    -   access.date
    -   site

2.  sample.meta

3.  taxon.list

4.  counts

5.  lab.data

6.  chronologies

Within the download object, `sample.meta` stores the core depth and age information for that dataset. We just want to look at the first few lines, so are using the `head()` function. Let's explore different facets of the dataset

```{r seemauletaxon, echo=TRUE, eval = FALSE}
head(maule_pd[[1]]$sample.meta)
#taxon.list stores a list of taxa found in the dataset
head(maule_pd[[1]]$taxon.list)
#counts stores the the counts, presence/absence data, or percentage data for each taxon for each sample
head(maule_pd[[1]]$counts)
```

Here we can get to the papers that published about a site of interest:

```{r  seemaulepub, echo=TRUE, eval = FALSE}
get_publication(datasetid = 19817)
```

# Multi-Site Analysis
